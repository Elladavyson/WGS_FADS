### Using the UKB RAP 
# - Download the DNAnexus toolkit (local computer)
# - Set up the ssh-config using dx ssh-config
# - Launch a Cloud Workstation (from UKB-RAP interface)
# - Inspect it on the UKB-RAP interface (to get Job ID)

# DNA Nexus
# Run the login command first 
dx login 
# Launch the workstation from command line 
dx run app-cloud_workstation --ssh
# Log in to the launched CloudStation (Job ID)
dx ssh job-ID

# Cloud Computing UKB links 
# https://www.youtube.com/watch?v=9I78As5uRfw - youtube video about cloud computing at UKB RAP 
# https://documentation.dnanexus.com/developer/cloud-workstation

# Setting up the workstation 
unset DX_WORKSPACE_ID # Unsets an environment variable set up when the cloudstation is launched
dx cd $DX_PROJECT_CONTEXT_ID: # Change the workstations working directory to that of the parent project 

# View and select working project
dx select --level=VIEW
# Downloading data to the workstation 
# If you've set the project using the dx select above then you don't need the project: 
dx download project-WGS_AVT:/Bulk/DRAGEN\ WGS/DRAGEN\ population\ level\ WGS\ variants,\ pVCF\ format\ [500k\ release]/chr11/ukb24310_c11_b3080_v1.vcf.gz*

# Downloading BCFTOOLS
git clone --recurse-submodules https://github.com/samtools/htslib.git
git clone https://github.com/samtools/bcftools.git
cd bcftools
make

# Downloading VCFTOOLS
# Does not work on the cloud as you need sudo to perform make install
# All VCFtools demands submitted through SAK

git clone https://github.com/vcftools/vcftools.git
cd vcftools
./autogen.sh
./configure
make
make install

# Add to the PATH variable 
export PATH="$PATH:~/bcftools"
export PATH="$PATH:~/vcftools"
export BCFTOOLS_PLUGINS=~/bcftools/plugins
# Save the entire workspace on the worker as a snapshot
dx-create-snapshot

# BCFTools look at the top position of the file 
bcftools view -H ukb24310_c11_b3080_v1.vcf.gz | view 


# Variant annotation links 
# https://www.youtube.com/watch?v=mK_204Zvkbs

# Uploading any files back to work project

dx upload --path "$DX_PROJECT_CONTEXT_ID:" file 

### Cohort browswer

# Create a cohort using the filtering metrics and add the columns you want (unsure how you can do this programmatically)
# Then convert to a csv file using the Table exporter tool

### Lifting over all the genotype array data to GRCh38 (from GRCh37) - not certain if neccessary for regenie
### Mark wasn't sure it was but all the tutorials about the analysis of WES say that it is 

# download the workflow from Github: 
https://github.com/dnanexus-rnd/liftover_plink_beds/blob/main/liftover_plink_beds.wdl
#download java locally: 
brew install openjdk
#download the latest JAR compiler:
https://github.com/dnanexus/dxCompiler/releases
# Login 
dx login 
# Compile the workflow
java -jar Downloads/dxCompiler-2.11.9.jar compile Downloads/liftover_plink_beds.wdl -project WGS_AVT -folder /Output/

# Run the workflow with the following inputs 

plink_beds: /Bulk/Genotype Results/Genotype calls/*.bed (22 files)

plink_bims: /Bulk/Genotype Results/Genotype calls/*.bim (22 files)

plink_fams: /Bulk/Genotype Results/Genotype calls/*.fam (22 files)

reference_fastagz: /Bulk/Exome sequences/Exome OQFE CRAM files/helper_files/GRCh38_full_analysis_set_plus_decoy_hla.fa
# Chain file downloaded from the UCSC server
ucsc_chain: hg19ToHg38.over.chain.gz 
